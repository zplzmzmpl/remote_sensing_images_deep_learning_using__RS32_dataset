{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9625b45b",
   "metadata": {},
   "source": [
    "<div align='center' ><font size='6'>pytorch框架下实现卷积神经网络分类</font></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81967ba",
   "metadata": {},
   "source": [
    "## 实习要求：\n",
    "\n",
    "使用pytorch框架的模块完成任务：\n",
    "\n",
    "\n",
    "**一、使用提供的三种卷积网络形式对HelloRS32数据集分类**\n",
    "  1. model1：编写并实现简单的双层卷积神经网络的训练及预测，精度达到**86%**以上\n",
    "  \n",
    "  \n",
    "  2. model2：编写并实现三层卷积神经网络的训练及预测，精度达到**88%**以上\n",
    "      * 使用了relu作为激活函数\n",
    "      * 在线性层中加入dropout\n",
    "      * 使用局部响应归一化层lrn\n",
    "      * 相比与modle1拥有更深的卷积层\n",
    "      \n",
    "      \n",
    "  3. model3：编写并实现很多层的卷积神经网络的训练及预测，精度达到**91%**以上\n",
    "      * 更小的卷积核尺寸、卷积步长\n",
    "      * 使用批量归一化batchnorm代替局部响应归一化层lrn\n",
    "      * 更深的网络\n",
    "  \n",
    "  \n",
    "   4. **选做**：尝试在model3的基础上加入skip connection，进一步提升精度\n",
    "\n",
    "**二、完成神经网络网络在cifar-10上的训练、预测函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb73060",
   "metadata": {},
   "source": [
    "# 使用pytorch框架的模块完成任务\n",
    "\n",
    "Dropout、Batch Norm和2D卷积是计算机视觉中深度学习的主要工具。\n",
    "\n",
    "在本作业的最后一部分，放弃之前的代码库，转而迁移到流行的深度学习框架之一: PyTorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f7631",
   "metadata": {},
   "source": [
    "## PyTorch是什么?\n",
    "\n",
    "PyTorch是一个在Tensor对象上执行动态计算图形的系统，其行为类似于numpy ndarray。它带有一个强大的自动区分引擎，消除了手动反向传播的需要。\n",
    "\n",
    "### 为什么采用Pytorch\n",
    "\n",
    "* 我们的代码现在将在gpu上运行速度更快的训练。当使用像PyTorch或TensorFlow这样的框架时，你可以为你自己的自定义神经网络架构利用GPU的能力，而不必直接编写CUDA代码(这超出了本类的范围)。\n",
    "* 我们希望你准备好在你的项目中使用这些框架之一，这样你就可以更有效地进行试验，而不是手工编写你想要使用的所有功能。\n",
    "* 我们要你站在巨人的肩膀上！PyTorch是很棒的框架，会让你的生活变得更简单，现在你已经理解了它的本质，你可以自由地使用它了:)\n",
    "* 我们希望你能接触到你在学术界或工业界可能会遇到的那种深度学习代码。\n",
    "\n",
    "### PyTorch版本\n",
    "\n",
    "假设你正在使用**PyTorch 1.0版本**。在之前的一些版本中(比如0.4之前)，Tensor必须被包装在变量对象中才能在autograd中使用；然而变量现在已被弃用。此外，1.0还将Tensor的数据类型从它的设备中分离出来，并使用numpy风格的工厂来构造Tensor，而不是直接调用Tensor构造函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34768a90",
   "metadata": {},
   "source": [
    "## 我将如何学习PyTorch?\n",
    "\n",
    "[PyTorch 向导](https://github.com/jcjohnson/pytorch-examples) 可以参考github的这些样例。\n",
    "\n",
    "也可以看Pytorch详细的 [API文档](http://pytorch.org/docs/stable/index.html)。如果你有其他API文档没有解决的问题, [PyTorch forum](https://discuss.pytorch.org/) 也是一个可以参考的地方。\n",
    "\n",
    "\n",
    "### PyTorch可以分为**三个不同的抽象层次**进行构建：\n",
    "1. Barebone PyTorch：**抽象级别1**，直接使用最低级别的PyTorch张量。\n",
    "2. PyTorch模块API：**抽象级别2**，使用`nn.Module`定义任意神经网络结构。\n",
    "3. PyTorch顺序API：**抽象级别3**，使用`nn.Sequential`可以非常方便地定义线性前馈网络。\n",
    "\n",
    "\n",
    "以下是一个比较表：\n",
    "\n",
    "| API           | 灵活性 | 方便性 |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      |高        | 低         |\n",
    "| `nn.Module`     | 高       | 中      |\n",
    "| `nn.Sequential` | 低         | 高        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baa4bf",
   "metadata": {},
   "source": [
    "# Part I. 准备阶段\n",
    "\n",
    "首先，加载HelloRS32数据集，对其进行预处理，并以mini_batch遍历它；PyTorch为我们提供了方便的工具来自动化这个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e260422",
   "metadata": {},
   "source": [
    "## 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79092777",
   "metadata": {},
   "source": [
    "## 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6561d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  \n",
    "lr = 0.001 \n",
    "epoch = 40\n",
    "print_every = 1000  # 输出损失的频率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6461ddf",
   "metadata": {},
   "source": [
    "## HelloRS32数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06da67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_path = r\"E:\\python\\jupyter\\lab1\\Quiz\\RS32\\train\"\n",
    "val_path = r\"E:\\python\\jupyter\\lab1\\Quiz\\RS32\\val\"\n",
    "test_path = r\"E:\\python\\jupyter\\lab1\\Quiz\\RS32\\test\"\n",
    "out_dir = r\"E:\\python\\jupyter\\lab3\\Quiz\\model\"\n",
    "\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0, 0, 0], [1, 1, 1])]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0, 0, 0], [1, 1, 1])]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0, 0, 0], [1, 1, 1])])}\n",
    "\n",
    "trainset = ImageFolder(train_path, transform=data_transform[\"train\"])\n",
    "valset = ImageFolder(val_path, transform=data_transform[\"val\"])\n",
    "testset = ImageFolder(test_path, transform=data_transform[\"test\"])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(valset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da0f15",
   "metadata": {},
   "source": [
    "你可以**通过设置下面的flag为True使用GPU**。本课程的任务可以不需要使用GPU，但使用GPU可以加速计算。请注意，如果你的计算机没有启用CUDA，`torch.cuda.is_available()`将返回False，将回退到CPU模式。\n",
    "全局变量`dtype `和`device`将控制整个赋值过程中的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf20aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32  #使用float作为数据类型\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95403df0-db4c-4aed-9fa7-467518632ec6",
   "metadata": {},
   "source": [
    "# Part II. 模型的设置\n",
    "新建 `model(*).py`,参照给定的网络构造实现不同卷积神经网络对遥感数据集HelloRS32进行分类：\n",
    "\n",
    "你需要查看 `torch.nn` 和 `torch.nn.functional` 来了解pytorch中卷积、池化、激活函数、全连接等功能是如何实现的。\n",
    "\n",
    "当你成功实现model之后，你可以尝试使用更多网络，如`resnet`、`vggnet`、`mobilenet`、`efficientnet`等更强大的网络来在HelloRS32上训练，以提高你的精度，当然这也意味着需要更大的算力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9beab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "import models done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import train\n",
    "import models\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('import models done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04074b",
   "metadata": {},
   "source": [
    "# Part III. PyTorch的训练\n",
    "\n",
    "在开始网络的训练之前，我们还需要设置网络的损失函数、优化器，这些设置对于网络的训练也具有重要的影响，但本课程暂不要求对这些设置具有深刻的理解，我们会给出一些示例，你在后续的训练中可以更换不同的损失函数、优化器以及`scheduler`，以提高训练的精度和效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45344e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "preparation done\n"
     ]
    }
   ],
   "source": [
    "net01 = models.Model1().to(device)\n",
    "print('load model done')\n",
    "optimizer01 = optim.SGD(net01.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler01 = torch.optim.lr_scheduler.StepLR(optimizer01, step_size=20, gamma=0.5)\n",
    "print('preparation done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defbd89",
   "metadata": {},
   "source": [
    "完成前期的设置之后，我们就可以开始编写训练的代码了，打开`train.py`，完成`train`和`test`函数，并在下面运行训练和测试的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5bf37f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1 | Batch: 1000/1928 | Loss: 1.473 | Acc: 46.125%\n",
      "val Loss: 1.196 | Acc: 57.974%\n",
      "Saving..\n",
      "Epoch: 1\n",
      "Epoch: 2 | Batch: 1000/1928 | Loss: 1.092 | Acc: 54.095%\n",
      "val Loss: 1.079 | Acc: 58.573%\n",
      "Saving..\n",
      "Epoch: 2\n",
      "Epoch: 3 | Batch: 1000/1928 | Loss: 0.999 | Acc: 57.508%\n",
      "val Loss: 0.945 | Acc: 61.005%\n",
      "Saving..\n",
      "Epoch: 3\n",
      "Epoch: 4 | Batch: 1000/1928 | Loss: 0.941 | Acc: 59.808%\n",
      "val Loss: 0.902 | Acc: 61.842%\n",
      "Saving..\n",
      "Epoch: 4\n",
      "Epoch: 5 | Batch: 1000/1928 | Loss: 0.841 | Acc: 61.583%\n",
      "val Loss: 0.877 | Acc: 63.206%\n",
      "Saving..\n",
      "Epoch: 5\n",
      "Epoch: 6 | Batch: 1000/1928 | Loss: 0.807 | Acc: 63.204%\n",
      "val Loss: 0.799 | Acc: 64.740%\n",
      "Saving..\n",
      "Epoch: 6\n",
      "Epoch: 7 | Batch: 1000/1928 | Loss: 0.748 | Acc: 64.488%\n",
      "val Loss: 0.809 | Acc: 65.710%\n",
      "Saving..\n",
      "Epoch: 7\n",
      "Epoch: 8 | Batch: 1000/1928 | Loss: 0.726 | Acc: 65.614%\n",
      "val Loss: 0.750 | Acc: 66.766%\n",
      "Saving..\n",
      "Epoch: 8\n",
      "Epoch: 9 | Batch: 1000/1928 | Loss: 0.704 | Acc: 66.561%\n",
      "val Loss: 0.742 | Acc: 67.588%\n",
      "Saving..\n",
      "Epoch: 9\n",
      "Epoch: 10 | Batch: 1000/1928 | Loss: 0.656 | Acc: 67.458%\n",
      "val Loss: 0.747 | Acc: 68.054%\n",
      "Saving..\n",
      "Epoch: 10\n",
      "Epoch: 11 | Batch: 1000/1928 | Loss: 0.529 | Acc: 68.408%\n",
      "val Loss: 0.656 | Acc: 68.914%\n",
      "Saving..\n",
      "Epoch: 11\n",
      "Epoch: 12 | Batch: 1000/1928 | Loss: 0.501 | Acc: 69.473%\n",
      "val Loss: 0.626 | Acc: 69.763%\n",
      "Saving..\n",
      "Epoch: 12\n",
      "Epoch: 13 | Batch: 1000/1928 | Loss: 0.489 | Acc: 70.419%\n",
      "val Loss: 0.694 | Acc: 70.372%\n",
      "Saving..\n",
      "Epoch: 13\n",
      "Epoch: 14 | Batch: 1000/1928 | Loss: 0.462 | Acc: 71.247%\n",
      "val Loss: 0.586 | Acc: 71.013%\n",
      "Saving..\n",
      "Epoch: 14\n",
      "Epoch: 15 | Batch: 1000/1928 | Loss: 0.451 | Acc: 72.000%\n",
      "val Loss: 0.655 | Acc: 71.627%\n",
      "Saving..\n",
      "Epoch: 15\n",
      "Epoch: 16 | Batch: 1000/1928 | Loss: 0.477 | Acc: 72.659%\n",
      "val Loss: 0.638 | Acc: 72.144%\n",
      "Saving..\n",
      "Epoch: 16\n",
      "Epoch: 17 | Batch: 1000/1928 | Loss: 0.473 | Acc: 73.254%\n",
      "val Loss: 0.660 | Acc: 72.455%\n",
      "Saving..\n",
      "Epoch: 17\n",
      "Epoch: 18 | Batch: 1000/1928 | Loss: 0.453 | Acc: 73.795%\n",
      "val Loss: 0.651 | Acc: 72.656%\n",
      "Saving..\n",
      "Epoch: 18\n",
      "Epoch: 19 | Batch: 1000/1928 | Loss: 0.437 | Acc: 74.309%\n",
      "val Loss: 0.631 | Acc: 73.004%\n",
      "Saving..\n",
      "Epoch: 19\n",
      "Epoch: 20 | Batch: 1000/1928 | Loss: 0.417 | Acc: 74.777%\n",
      "val Loss: 0.638 | Acc: 73.130%\n",
      "Saving..\n",
      "Epoch: 20\n",
      "Epoch: 21 | Batch: 1000/1928 | Loss: 0.351 | Acc: 75.252%\n",
      "val Loss: 0.541 | Acc: 73.532%\n",
      "Saving..\n",
      "Epoch: 21\n",
      "Epoch: 22 | Batch: 1000/1928 | Loss: 0.348 | Acc: 75.807%\n",
      "val Loss: 0.551 | Acc: 73.858%\n",
      "Saving..\n",
      "Epoch: 22\n",
      "Epoch: 23 | Batch: 1000/1928 | Loss: 0.320 | Acc: 76.325%\n",
      "val Loss: 0.591 | Acc: 74.183%\n",
      "Saving..\n",
      "Epoch: 23\n",
      "Epoch: 24 | Batch: 1000/1928 | Loss: 0.343 | Acc: 76.780%\n",
      "val Loss: 0.634 | Acc: 74.428%\n",
      "Saving..\n",
      "Epoch: 24\n",
      "Epoch: 25 | Batch: 1000/1928 | Loss: 0.329 | Acc: 77.225%\n",
      "val Loss: 0.556 | Acc: 74.718%\n",
      "Saving..\n",
      "Epoch: 25\n",
      "Epoch: 26 | Batch: 1000/1928 | Loss: 0.320 | Acc: 77.641%\n",
      "val Loss: 0.578 | Acc: 75.034%\n",
      "Saving..\n",
      "Epoch: 26\n",
      "Epoch: 27 | Batch: 1000/1928 | Loss: 0.308 | Acc: 78.041%\n",
      "val Loss: 0.593 | Acc: 75.270%\n",
      "Saving..\n",
      "Epoch: 27\n",
      "Epoch: 28 | Batch: 1000/1928 | Loss: 0.306 | Acc: 78.412%\n",
      "val Loss: 0.602 | Acc: 75.422%\n",
      "Saving..\n",
      "Epoch: 28\n",
      "Epoch: 29 | Batch: 1000/1928 | Loss: 0.307 | Acc: 78.739%\n",
      "val Loss: 0.667 | Acc: 75.499%\n",
      "Saving..\n",
      "Epoch: 29\n",
      "Epoch: 30 | Batch: 1000/1928 | Loss: 0.297 | Acc: 79.069%\n",
      "val Loss: 0.581 | Acc: 75.731%\n",
      "Saving..\n",
      "Epoch: 30\n",
      "Epoch: 31 | Batch: 1000/1928 | Loss: 0.264 | Acc: 79.389%\n",
      "val Loss: 0.557 | Acc: 75.961%\n",
      "Saving..\n",
      "Epoch: 31\n",
      "Epoch: 32 | Batch: 1000/1928 | Loss: 0.255 | Acc: 79.741%\n",
      "val Loss: 0.610 | Acc: 76.057%\n",
      "Saving..\n",
      "Epoch: 32\n",
      "Epoch: 33 | Batch: 1000/1928 | Loss: 0.259 | Acc: 80.066%\n",
      "val Loss: 0.567 | Acc: 76.251%\n",
      "Saving..\n",
      "Epoch: 33\n",
      "Epoch: 34 | Batch: 1000/1928 | Loss: 0.248 | Acc: 80.383%\n",
      "val Loss: 0.569 | Acc: 76.447%\n",
      "Saving..\n",
      "Epoch: 34\n",
      "Epoch: 35 | Batch: 1000/1928 | Loss: 0.268 | Acc: 80.667%\n",
      "val Loss: 0.600 | Acc: 76.626%\n",
      "Saving..\n",
      "Epoch: 35\n",
      "Epoch: 36 | Batch: 1000/1928 | Loss: 0.259 | Acc: 80.956%\n",
      "val Loss: 0.637 | Acc: 76.757%\n",
      "Saving..\n",
      "Epoch: 36\n",
      "Epoch: 37 | Batch: 1000/1928 | Loss: 0.244 | Acc: 81.230%\n",
      "val Loss: 0.600 | Acc: 76.883%\n",
      "Saving..\n",
      "Epoch: 37\n",
      "Epoch: 38 | Batch: 1000/1928 | Loss: 0.234 | Acc: 81.488%\n",
      "val Loss: 0.604 | Acc: 77.008%\n",
      "Saving..\n",
      "Epoch: 38\n",
      "Epoch: 39 | Batch: 1000/1928 | Loss: 0.252 | Acc: 81.727%\n",
      "val Loss: 0.587 | Acc: 77.162%\n",
      "Saving..\n",
      "Epoch: 39\n",
      "Epoch: 40 | Batch: 1000/1928 | Loss: 0.245 | Acc: 81.963%\n",
      "val Loss: 0.629 | Acc: 77.301%\n",
      "Saving..\n",
      "2261.95170211792\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "train.train_val(train_loader, val_loader, net01, criterion, optimizer01, epoch, print_every, device, scheduler01,\n",
    "            output_dir=out_dir+'\\01')\n",
    "\n",
    "toc = time.time()\n",
    "print('time consuming:',toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72144221",
   "metadata": {},
   "source": [
    "## 测试集\n",
    "\n",
    "我们通过训练得到了精度最高的模型之后，在测试集上运行我们的模型，并得到测试精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175d41c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Loss: 0.590 | Acc: 83.142%\n"
     ]
    }
   ],
   "source": [
    "#加载模型参数\n",
    "ckp = r\"E:\\python\\jupyter\\lab3\\Quiz\\model\\01\\epoch039.pth\"\n",
    "pretrained_dict = torch.load(ckp)\n",
    "net01.load_state_dict(pretrained_dict[\"net\"])\n",
    "\n",
    "#测试\n",
    "train.predict(test_loader, net, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e4dc383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "Epoch: 1 | Batch: 1000/1928 | Loss: 1.695 | Acc: 37.200%\n",
      "val Loss: 1.016 | Acc: 62.998%\n",
      "Saving model...\n",
      "Epoch: 2 | Batch: 1000/1928 | Loss: 0.933 | Acc: 53.873%\n",
      "val Loss: 0.779 | Acc: 68.740%\n",
      "Saving model...\n",
      "Epoch: 3 | Batch: 1000/1928 | Loss: 0.749 | Acc: 60.871%\n",
      "val Loss: 0.575 | Acc: 71.212%\n",
      "Saving model...\n",
      "Epoch: 4 | Batch: 1000/1928 | Loss: 0.603 | Acc: 65.639%\n",
      "val Loss: 0.504 | Acc: 73.923%\n",
      "Saving model...\n",
      "Epoch: 5 | Batch: 1000/1928 | Loss: 0.481 | Acc: 68.742%\n",
      "val Loss: 0.436 | Acc: 75.901%\n",
      "Saving model...\n",
      "Epoch: 6 | Batch: 1000/1928 | Loss: 0.480 | Acc: 70.946%\n",
      "val Loss: 0.708 | Acc: 75.917%\n",
      "Saving model...\n",
      "Epoch: 7 | Batch: 1000/1928 | Loss: 0.421 | Acc: 72.955%\n",
      "val Loss: 0.588 | Acc: 76.612%\n",
      "Saving model...\n",
      "Epoch: 8 | Batch: 1000/1928 | Loss: 0.383 | Acc: 74.627%\n",
      "val Loss: 0.517 | Acc: 77.163%\n",
      "Saving model...\n",
      "Epoch: 9 | Batch: 1000/1928 | Loss: 0.308 | Acc: 76.159%\n",
      "val Loss: 0.536 | Acc: 77.441%\n",
      "Saving model...\n",
      "Epoch: 10 | Batch: 1000/1928 | Loss: 0.276 | Acc: 77.419%\n",
      "val Loss: 0.530 | Acc: 77.911%\n",
      "Saving model...\n",
      "Epoch: 11 | Batch: 1000/1928 | Loss: 0.183 | Acc: 78.652%\n",
      "val Loss: 0.312 | Acc: 78.904%\n",
      "Saving model...\n",
      "Epoch: 12 | Batch: 1000/1928 | Loss: 0.113 | Acc: 80.111%\n",
      "val Loss: 0.354 | Acc: 79.725%\n",
      "Saving model...\n",
      "Epoch: 13 | Batch: 1000/1928 | Loss: 0.096 | Acc: 81.402%\n",
      "val Loss: 0.382 | Acc: 80.456%\n",
      "Saving model...\n",
      "Epoch: 14 | Batch: 1000/1928 | Loss: 0.070 | Acc: 82.593%\n",
      "val Loss: 0.428 | Acc: 80.952%\n",
      "Saving model...\n",
      "Epoch: 15 | Batch: 1000/1928 | Loss: 0.061 | Acc: 83.635%\n",
      "val Loss: 0.354 | Acc: 81.600%\n",
      "Saving model...\n",
      "Epoch: 16 | Batch: 1000/1928 | Loss: 0.053 | Acc: 84.565%\n",
      "val Loss: 0.628 | Acc: 81.813%\n",
      "Saving model...\n",
      "Epoch: 17 | Batch: 1000/1928 | Loss: 0.049 | Acc: 85.395%\n",
      "val Loss: 0.457 | Acc: 82.236%\n",
      "Saving model...\n",
      "Epoch: 18 | Batch: 1000/1928 | Loss: 0.032 | Acc: 86.152%\n",
      "val Loss: 0.446 | Acc: 82.593%\n",
      "Saving model...\n",
      "Epoch: 19 | Batch: 1000/1928 | Loss: 0.038 | Acc: 86.833%\n",
      "val Loss: 0.511 | Acc: 82.914%\n",
      "Saving model...\n",
      "Epoch: 20 | Batch: 1000/1928 | Loss: 0.052 | Acc: 87.425%\n",
      "val Loss: 0.491 | Acc: 83.254%\n",
      "Saving model...\n",
      "Epoch: 21 | Batch: 1000/1928 | Loss: 0.035 | Acc: 87.968%\n",
      "val Loss: 0.427 | Acc: 83.607%\n",
      "Saving model...\n",
      "Epoch: 22 | Batch: 1000/1928 | Loss: 0.019 | Acc: 88.508%\n",
      "val Loss: 0.407 | Acc: 83.950%\n",
      "Saving model...\n",
      "Epoch: 23 | Batch: 1000/1928 | Loss: 0.015 | Acc: 88.998%\n",
      "val Loss: 0.400 | Acc: 84.245%\n",
      "Saving model...\n",
      "Epoch: 24 | Batch: 1000/1928 | Loss: 0.017 | Acc: 89.448%\n",
      "val Loss: 0.419 | Acc: 84.506%\n",
      "Saving model...\n",
      "Epoch: 25 | Batch: 1000/1928 | Loss: 0.015 | Acc: 89.864%\n",
      "val Loss: 0.428 | Acc: 84.759%\n",
      "Saving model...\n",
      "Epoch: 26 | Batch: 1000/1928 | Loss: 0.010 | Acc: 90.250%\n",
      "val Loss: 0.416 | Acc: 84.996%\n",
      "Saving model...\n",
      "Epoch: 27 | Batch: 1000/1928 | Loss: 0.014 | Acc: 90.610%\n",
      "val Loss: 0.423 | Acc: 85.221%\n",
      "Saving model...\n",
      "Epoch: 28 | Batch: 1000/1928 | Loss: 0.012 | Acc: 90.942%\n",
      "val Loss: 0.418 | Acc: 85.438%\n",
      "Saving model...\n",
      "Epoch: 29 | Batch: 1000/1928 | Loss: 0.013 | Acc: 91.250%\n",
      "val Loss: 0.437 | Acc: 85.605%\n",
      "Saving model...\n",
      "Epoch: 30 | Batch: 1000/1928 | Loss: 0.017 | Acc: 91.535%\n",
      "val Loss: 0.429 | Acc: 85.779%\n",
      "Saving model...\n",
      "Epoch: 31 | Batch: 1000/1928 | Loss: 0.010 | Acc: 91.807%\n",
      "val Loss: 0.442 | Acc: 85.944%\n",
      "Saving model...\n",
      "Epoch: 32 | Batch: 1000/1928 | Loss: 0.011 | Acc: 92.062%\n",
      "val Loss: 0.428 | Acc: 86.097%\n",
      "Saving model...\n",
      "Epoch: 33 | Batch: 1000/1928 | Loss: 0.009 | Acc: 92.303%\n",
      "val Loss: 0.431 | Acc: 86.238%\n",
      "Saving model...\n",
      "Epoch: 34 | Batch: 1000/1928 | Loss: 0.009 | Acc: 92.526%\n",
      "val Loss: 0.426 | Acc: 86.382%\n",
      "Saving model...\n",
      "Epoch: 35 | Batch: 1000/1928 | Loss: 0.008 | Acc: 92.736%\n",
      "val Loss: 0.425 | Acc: 86.512%\n",
      "Saving model...\n",
      "Epoch: 36 | Batch: 1000/1928 | Loss: 0.007 | Acc: 92.938%\n",
      "val Loss: 0.431 | Acc: 86.629%\n",
      "Saving model...\n",
      "Epoch: 37 | Batch: 1000/1928 | Loss: 0.009 | Acc: 93.128%\n",
      "val Loss: 0.431 | Acc: 86.747%\n",
      "Saving model...\n",
      "Epoch: 38 | Batch: 1000/1928 | Loss: 0.008 | Acc: 93.309%\n",
      "val Loss: 0.435 | Acc: 86.855%\n",
      "Saving model...\n",
      "Epoch: 39 | Batch: 1000/1928 | Loss: 0.008 | Acc: 93.479%\n",
      "val Loss: 0.429 | Acc: 86.967%\n",
      "Saving model...\n",
      "Epoch: 40 | Batch: 1000/1928 | Loss: 0.008 | Acc: 93.643%\n",
      "val Loss: 0.424 | Acc: 87.075%\n",
      "Saving model...\n",
      "time consuming: 1854.1995975971222\n"
     ]
    }
   ],
   "source": [
    "net02 = models.Model2().to(device)\n",
    "optimizer02 = optim.SGD(net02.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler02 = torch.optim.lr_scheduler.StepLR(optimizer02, step_size=20, gamma=0.5)\n",
    "print('load model done')\n",
    "tic = time.time()\n",
    "train.train_val(train_loader, val_loader, net02, criterion, optimizer02, epoch, print_every, device, scheduler02,\n",
    "            output_dir=out_dir+'\\\\02')\n",
    "toc = time.time()\n",
    "print('time consuming:',toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfdd1fa-58a7-421b-920f-8226d026b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Loss: 0.450 | Acc: 90.803%\n"
     ]
    }
   ],
   "source": [
    "#加载模型参数\n",
    "ckp = r\"E:\\python\\jupyter\\lab3\\Quiz\\model\\02\\best_model.pth\"\n",
    "pretrained_dict = torch.load(ckp)\n",
    "net02.load_state_dict(pretrained_dict[\"net\"])\n",
    "\n",
    "#测试\n",
    "train.predict(test_loader, net02, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de97b56-bd1a-4f0e-aec7-fcb96eb7f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "Epoch: 1 | Batch: 1000/1928 | Loss: 1.682 | Acc: 39.425%\n",
      "val Loss: 1.048 | Acc: 58.612%\n",
      "Saving model...\n",
      "Epoch: 2 | Batch: 1000/1928 | Loss: 1.016 | Acc: 53.531%\n",
      "val Loss: 0.888 | Acc: 64.474%\n",
      "Saving model...\n",
      "Epoch: 3 | Batch: 1000/1928 | Loss: 0.770 | Acc: 60.058%\n",
      "val Loss: 0.545 | Acc: 70.016%\n",
      "Saving model...\n",
      "Epoch: 4 | Batch: 1000/1928 | Loss: 0.609 | Acc: 64.968%\n",
      "val Loss: 0.556 | Acc: 72.887%\n",
      "Saving model...\n",
      "Epoch: 5 | Batch: 1000/1928 | Loss: 0.520 | Acc: 68.461%\n",
      "val Loss: 0.481 | Acc: 75.247%\n",
      "Saving model...\n",
      "Epoch: 6 | Batch: 1000/1928 | Loss: 0.477 | Acc: 71.223%\n",
      "val Loss: 0.486 | Acc: 76.568%\n",
      "Saving model...\n",
      "Epoch: 7 | Batch: 1000/1928 | Loss: 0.403 | Acc: 73.552%\n",
      "val Loss: 0.420 | Acc: 77.899%\n",
      "Saving model...\n",
      "Epoch: 8 | Batch: 1000/1928 | Loss: 0.373 | Acc: 75.314%\n",
      "val Loss: 0.477 | Acc: 78.668%\n",
      "Saving model...\n",
      "Epoch: 9 | Batch: 1000/1928 | Loss: 0.318 | Acc: 76.885%\n",
      "val Loss: 0.417 | Acc: 79.506%\n",
      "Saving model...\n",
      "Epoch: 10 | Batch: 1000/1928 | Loss: 0.284 | Acc: 78.305%\n",
      "val Loss: 0.414 | Acc: 80.311%\n",
      "Saving model...\n",
      "Epoch: 11 | Batch: 1000/1928 | Loss: 0.164 | Acc: 79.667%\n",
      "val Loss: 0.320 | Acc: 81.216%\n",
      "Saving model...\n",
      "Epoch: 12 | Batch: 1000/1928 | Loss: 0.123 | Acc: 81.047%\n",
      "val Loss: 0.361 | Acc: 81.964%\n",
      "Saving model...\n",
      "Epoch: 13 | Batch: 1000/1928 | Loss: 0.108 | Acc: 82.257%\n",
      "val Loss: 0.320 | Acc: 82.597%\n",
      "Saving model...\n",
      "Epoch: 14 | Batch: 1000/1928 | Loss: 0.076 | Acc: 83.326%\n",
      "val Loss: 0.350 | Acc: 83.145%\n",
      "Saving model...\n",
      "Epoch: 15 | Batch: 1000/1928 | Loss: 0.089 | Acc: 84.264%\n",
      "val Loss: 0.359 | Acc: 83.599%\n",
      "Saving model...\n",
      "Epoch: 16 | Batch: 1000/1928 | Loss: 0.086 | Acc: 85.101%\n",
      "val Loss: 0.419 | Acc: 83.976%\n",
      "Saving model...\n",
      "Epoch: 17 | Batch: 1000/1928 | Loss: 0.063 | Acc: 85.862%\n",
      "val Loss: 0.390 | Acc: 84.309%\n",
      "Saving model...\n",
      "Epoch: 18 | Batch: 1000/1928 | Loss: 0.071 | Acc: 86.525%\n",
      "val Loss: 0.354 | Acc: 84.658%\n",
      "Saving model...\n",
      "Epoch: 19 | Batch: 1000/1928 | Loss: 0.064 | Acc: 87.135%\n",
      "val Loss: 0.363 | Acc: 84.962%\n",
      "Saving model...\n",
      "Epoch: 20 | Batch: 1000/1928 | Loss: 0.047 | Acc: 87.695%\n",
      "val Loss: 0.484 | Acc: 85.136%\n",
      "Saving model...\n",
      "Epoch: 21 | Batch: 1000/1928 | Loss: 0.042 | Acc: 88.203%\n",
      "val Loss: 0.399 | Acc: 85.357%\n",
      "Saving model...\n",
      "Epoch: 22 | Batch: 1000/1928 | Loss: 0.017 | Acc: 88.721%\n",
      "val Loss: 0.380 | Acc: 85.592%\n",
      "Saving model...\n",
      "Epoch: 23 | Batch: 1000/1928 | Loss: 0.012 | Acc: 89.211%\n",
      "val Loss: 0.381 | Acc: 85.805%\n",
      "Saving model...\n",
      "Epoch: 24 | Batch: 1000/1928 | Loss: 0.011 | Acc: 89.658%\n",
      "val Loss: 0.384 | Acc: 86.038%\n",
      "Saving model...\n",
      "Epoch: 25 | Batch: 1000/1928 | Loss: 0.009 | Acc: 90.066%\n",
      "val Loss: 0.388 | Acc: 86.249%\n",
      "Saving model...\n",
      "Epoch: 26 | Batch: 1000/1928 | Loss: 0.007 | Acc: 90.447%\n",
      "val Loss: 0.463 | Acc: 86.422%\n",
      "Saving model...\n",
      "Epoch: 27 | Batch: 1000/1928 | Loss: 0.011 | Acc: 90.795%\n",
      "val Loss: 0.422 | Acc: 86.559%\n",
      "Saving model...\n",
      "Epoch: 28 | Batch: 1000/1928 | Loss: 0.013 | Acc: 91.117%\n",
      "val Loss: 0.432 | Acc: 86.697%\n",
      "Saving model...\n",
      "Epoch: 29 | Batch: 1000/1928 | Loss: 0.005 | Acc: 91.420%\n",
      "val Loss: 0.452 | Acc: 86.823%\n",
      "Saving model...\n",
      "Epoch: 30 | Batch: 1000/1928 | Loss: 0.006 | Acc: 91.705%\n",
      "val Loss: 0.481 | Acc: 86.935%\n",
      "Saving model...\n",
      "Epoch: 31 | Batch: 1000/1928 | Loss: 0.005 | Acc: 91.971%\n",
      "val Loss: 0.471 | Acc: 87.050%\n",
      "Saving model...\n",
      "Epoch: 32 | Batch: 1000/1928 | Loss: 0.002 | Acc: 92.224%\n",
      "val Loss: 0.446 | Acc: 87.159%\n",
      "Saving model...\n",
      "Epoch: 33 | Batch: 1000/1928 | Loss: 0.003 | Acc: 92.461%\n",
      "val Loss: 0.475 | Acc: 87.263%\n",
      "Saving model...\n",
      "Epoch: 34 | Batch: 1000/1928 | Loss: 0.003 | Acc: 92.685%\n",
      "val Loss: 0.476 | Acc: 87.353%\n",
      "Saving model...\n",
      "Epoch: 35 | Batch: 1000/1928 | Loss: 0.002 | Acc: 92.896%\n",
      "val Loss: 0.454 | Acc: 87.437%\n",
      "Saving model...\n",
      "Epoch: 36 | Batch: 1000/1928 | Loss: 0.003 | Acc: 93.095%\n",
      "val Loss: 0.443 | Acc: 87.511%\n",
      "Saving model...\n",
      "Epoch: 37 | Batch: 1000/1928 | Loss: 0.004 | Acc: 93.282%\n",
      "val Loss: 0.471 | Acc: 87.584%\n",
      "Saving model...\n",
      "Epoch: 38 | Batch: 1000/1928 | Loss: 0.001 | Acc: 93.460%\n",
      "val Loss: 0.448 | Acc: 87.654%\n",
      "Saving model...\n",
      "Epoch: 39 | Batch: 1000/1928 | Loss: 0.001 | Acc: 93.629%\n",
      "val Loss: 0.446 | Acc: 87.734%\n",
      "Saving model...\n",
      "Epoch: 40 | Batch: 1000/1928 | Loss: 0.001 | Acc: 93.790%\n",
      "val Loss: 0.459 | Acc: 87.803%\n",
      "Saving model...\n",
      "time consuming: 1866.4473404884338\n"
     ]
    }
   ],
   "source": [
    "net03 = models.ConvNet().to(device)\n",
    "optimizer03 = optim.SGD(net03.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler03 = torch.optim.lr_scheduler.StepLR(optimizer03, step_size=20, gamma=0.5)\n",
    "print('load model done')\n",
    "\n",
    "tic = time.time()\n",
    "train.train_val(train_loader, val_loader, net03, criterion, optimizer03, epoch, print_every, device, scheduler03,\n",
    "            output_dir=out_dir+'\\\\03')\n",
    "\n",
    "toc = time.time()\n",
    "print('time consuming:',toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095066be-ea7a-446a-8d03-bd74854fa178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Loss: 0.354 | Acc: 92.705%\n"
     ]
    }
   ],
   "source": [
    "#加载模型参数\n",
    "ckp = r\"E:\\python\\jupyter\\lab3\\Quiz\\model\\03\\best_model.pth\"\n",
    "pretrained_dict = torch.load(ckp)\n",
    "net03.load_state_dict(pretrained_dict[\"net\"])\n",
    "\n",
    "#测试\n",
    "train.predict(test_loader, net03, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c78a3f7a-562f-441d-868e-e835ea263ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model done\n",
      "Epoch: 1 | Batch: 1000/1928 | Loss: 1.588 | Acc: 44.275%\n",
      "val Loss: 1.241 | Acc: 63.238%\n",
      "Saving model...\n",
      "Epoch: 2 | Batch: 1000/1928 | Loss: 0.837 | Acc: 57.853%\n",
      "val Loss: 0.778 | Acc: 68.301%\n",
      "Saving model...\n",
      "Epoch: 3 | Batch: 1000/1928 | Loss: 0.591 | Acc: 65.208%\n",
      "val Loss: 0.418 | Acc: 74.428%\n",
      "Saving model...\n",
      "Epoch: 4 | Batch: 1000/1928 | Loss: 0.409 | Acc: 70.531%\n",
      "val Loss: 0.604 | Acc: 76.156%\n",
      "Saving model...\n",
      "Epoch: 5 | Batch: 1000/1928 | Loss: 0.312 | Acc: 74.302%\n",
      "val Loss: 0.406 | Acc: 78.214%\n",
      "Saving model...\n",
      "Epoch: 6 | Batch: 1000/1928 | Loss: 0.242 | Acc: 77.264%\n",
      "val Loss: 0.364 | Acc: 79.745%\n",
      "Saving model...\n",
      "Epoch: 7 | Batch: 1000/1928 | Loss: 0.179 | Acc: 79.624%\n",
      "val Loss: 0.371 | Acc: 80.987%\n",
      "Saving model...\n",
      "Epoch: 8 | Batch: 1000/1928 | Loss: 0.152 | Acc: 81.549%\n",
      "val Loss: 0.313 | Acc: 82.187%\n",
      "Saving model...\n",
      "Epoch: 9 | Batch: 1000/1928 | Loss: 0.123 | Acc: 83.162%\n",
      "val Loss: 0.351 | Acc: 82.881%\n",
      "Saving model...\n",
      "Epoch: 10 | Batch: 1000/1928 | Loss: 0.095 | Acc: 84.563%\n",
      "val Loss: 0.381 | Acc: 83.405%\n",
      "Saving model...\n",
      "Epoch: 11 | Batch: 1000/1928 | Loss: 0.039 | Acc: 85.864%\n",
      "val Loss: 0.353 | Acc: 83.971%\n",
      "Saving model...\n",
      "Epoch: 12 | Batch: 1000/1928 | Loss: 0.026 | Acc: 87.018%\n",
      "val Loss: 0.301 | Acc: 84.603%\n",
      "Saving model...\n",
      "Epoch: 13 | Batch: 1000/1928 | Loss: 0.015 | Acc: 88.016%\n",
      "val Loss: 0.339 | Acc: 85.125%\n",
      "Saving model...\n",
      "Epoch: 14 | Batch: 1000/1928 | Loss: 0.014 | Acc: 88.878%\n",
      "val Loss: 0.302 | Acc: 85.578%\n",
      "Saving model...\n",
      "Epoch: 15 | Batch: 1000/1928 | Loss: 0.014 | Acc: 89.619%\n",
      "val Loss: 0.349 | Acc: 85.885%\n",
      "Saving model...\n",
      "Epoch: 16 | Batch: 1000/1928 | Loss: 0.011 | Acc: 90.271%\n",
      "val Loss: 0.352 | Acc: 86.149%\n",
      "Saving model...\n",
      "Epoch: 17 | Batch: 1000/1928 | Loss: 0.012 | Acc: 90.844%\n",
      "val Loss: 0.315 | Acc: 86.439%\n",
      "Saving model...\n",
      "Epoch: 18 | Batch: 1000/1928 | Loss: 0.011 | Acc: 91.355%\n",
      "val Loss: 0.302 | Acc: 86.745%\n",
      "Saving model...\n",
      "Epoch: 19 | Batch: 1000/1928 | Loss: 0.009 | Acc: 91.817%\n",
      "val Loss: 0.330 | Acc: 86.964%\n",
      "Saving model...\n",
      "Epoch: 20 | Batch: 1000/1928 | Loss: 0.007 | Acc: 92.225%\n",
      "val Loss: 0.311 | Acc: 87.177%\n",
      "Saving model...\n",
      "Epoch: 21 | Batch: 1000/1928 | Loss: 0.006 | Acc: 92.601%\n",
      "val Loss: 0.314 | Acc: 87.381%\n",
      "Saving model...\n",
      "Epoch: 22 | Batch: 1000/1928 | Loss: 0.007 | Acc: 92.942%\n",
      "val Loss: 0.302 | Acc: 87.592%\n",
      "Saving model...\n",
      "Epoch: 23 | Batch: 1000/1928 | Loss: 0.006 | Acc: 93.252%\n",
      "val Loss: 0.297 | Acc: 87.792%\n",
      "Saving model...\n",
      "Epoch: 24 | Batch: 1000/1928 | Loss: 0.005 | Acc: 93.536%\n",
      "val Loss: 0.314 | Acc: 87.942%\n",
      "Saving model...\n",
      "Epoch: 25 | Batch: 1000/1928 | Loss: 0.007 | Acc: 93.796%\n",
      "val Loss: 0.324 | Acc: 88.086%\n",
      "Saving model...\n",
      "Epoch: 26 | Batch: 1000/1928 | Loss: 0.005 | Acc: 94.036%\n",
      "val Loss: 0.323 | Acc: 88.235%\n",
      "Saving model...\n",
      "Epoch: 27 | Batch: 1000/1928 | Loss: 0.005 | Acc: 94.260%\n",
      "val Loss: 0.320 | Acc: 88.363%\n",
      "Saving model...\n",
      "Epoch: 28 | Batch: 1000/1928 | Loss: 0.006 | Acc: 94.465%\n",
      "val Loss: 0.305 | Acc: 88.491%\n",
      "Saving model...\n",
      "Epoch: 29 | Batch: 1000/1928 | Loss: 0.004 | Acc: 94.658%\n",
      "val Loss: 0.338 | Acc: 88.599%\n",
      "Saving model...\n",
      "Epoch: 30 | Batch: 1000/1928 | Loss: 0.003 | Acc: 94.837%\n",
      "val Loss: 0.316 | Acc: 88.708%\n",
      "Saving model...\n",
      "Epoch: 31 | Batch: 1000/1928 | Loss: 0.002 | Acc: 95.006%\n",
      "val Loss: 0.311 | Acc: 88.805%\n",
      "Saving model...\n",
      "Epoch: 32 | Batch: 1000/1928 | Loss: 0.002 | Acc: 95.163%\n",
      "val Loss: 0.309 | Acc: 88.891%\n",
      "Saving model...\n",
      "Epoch: 33 | Batch: 1000/1928 | Loss: 0.002 | Acc: 95.311%\n",
      "val Loss: 0.311 | Acc: 88.988%\n",
      "Saving model...\n",
      "Epoch: 34 | Batch: 1000/1928 | Loss: 0.003 | Acc: 95.450%\n",
      "val Loss: 0.317 | Acc: 89.073%\n",
      "Saving model...\n",
      "Epoch: 35 | Batch: 1000/1928 | Loss: 0.004 | Acc: 95.581%\n",
      "val Loss: 0.304 | Acc: 89.159%\n",
      "Saving model...\n",
      "Epoch: 36 | Batch: 1000/1928 | Loss: 0.002 | Acc: 95.704%\n",
      "val Loss: 0.301 | Acc: 89.241%\n",
      "Saving model...\n",
      "Epoch: 37 | Batch: 1000/1928 | Loss: 0.004 | Acc: 95.821%\n",
      "val Loss: 0.312 | Acc: 89.310%\n",
      "Saving model...\n",
      "Epoch: 38 | Batch: 1000/1928 | Loss: 0.003 | Acc: 95.931%\n",
      "val Loss: 0.320 | Acc: 89.386%\n",
      "Saving model...\n",
      "Epoch: 39 | Batch: 1000/1928 | Loss: 0.003 | Acc: 96.037%\n",
      "val Loss: 0.328 | Acc: 89.449%\n",
      "Saving model...\n",
      "Epoch: 40 | Batch: 1000/1928 | Loss: 0.002 | Acc: 96.137%\n",
      "val Loss: 0.304 | Acc: 89.520%\n",
      "Saving model...\n",
      "time consuming: 1877.9235997200012\n"
     ]
    }
   ],
   "source": [
    "net04 = models.Model4().to(device)\n",
    "optimizer04 = optim.SGD(net04.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler04 = torch.optim.lr_scheduler.StepLR(optimizer04, step_size=20, gamma=0.5)\n",
    "print('load model done')\n",
    "\n",
    "tic = time.time()\n",
    "train.train_val(train_loader, val_loader, net04, criterion, optimizer04, epoch, print_every, device, scheduler04,\n",
    "            output_dir=out_dir+'\\\\04')\n",
    "\n",
    "toc = time.time()\n",
    "print('time consuming:',toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e5bf058-9bd3-48b4-b4d6-fbbc22d17932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Loss: 0.233 | Acc: 93.590%\n"
     ]
    }
   ],
   "source": [
    "#加载模型参数\n",
    "ckp = r\"E:\\python\\jupyter\\lab3\\Quiz\\model\\04\\best_model.pth\"\n",
    "pretrained_dict = torch.load(ckp)\n",
    "net04.load_state_dict(pretrained_dict[\"net\"])\n",
    "\n",
    "#测试\n",
    "train.predict(test_loader, net04, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd3858-d44f-4e5e-89d8-8386c9fc16e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
